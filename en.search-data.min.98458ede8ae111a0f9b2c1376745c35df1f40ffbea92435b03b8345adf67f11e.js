'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/cloud-native-handbook/','title':"Cloud Native Handbook",'content':"Cloud Native Handbook The Cloud Native Handbook contains opinionated stances on Cloud Native concepts accompanied by exercises backed by a set of reference implementations. The reference code comes from on-the-job implementations and the opinions are based on experience consulting with companies ranging from the smallest of startups to large enterprises.\nTopics covered:\n Repository organization (code and config) Development workflow (branching, building container images) Deploying code and configuration changes (branching, gitops) Teams (delegating resource management)  "});index.add({'id':1,'href':'/docs/cloud-native-handbook/repositories/','title':"Repositories",'content':"Repositories When building on top of a cloud native platform all code and configuration should be version controlled. In Kubernetes, all deployment events are merely a result of updating a configuration file (referred to here as a Kubernetes manifest). Since all changes to code and config must be version controlled, we will use our repository as the origin of all changes to running software. This process is known as GitOps.\nWhen implementing a GitOps process, using a seperate repository for code and config (manifests) is recommended. The purpose of the code repo is solely to produce versioned software artifacts. The purpose of the config repo is to deploy software. Seperating these concerns provides multiple benefits detailed below.\nSmall Merges Multiple environments typically correspond to multiple git branches. By keeping the concept of deployments local to config repositories (and out of code repos) you are able to handle environment promotion with smaller merges. Config repos only need to merge a reference to a new container image tag rather than merging all of the code changes across N branches.\n Change Trail As a result of seperating development from deployment, any commit to a config repo represents a change to running software. This produces a legible, easily auditable change trail.\nLogical Organization of Components Applications are commonly composed of multiple components (REST APIs, databases, etc.). Some of these components may be built in-house while others may be from vendors or open source projects. A single config repo can provide this 1:N mapping of application to its components. This avoids the problem that arises when code and config co-exist in the same repositories: which repo should contain shared components?\nSimple Automation If code and deployment manifests live in the same repo, additional intelligence will be needed to determine if a code change or a config change occured in order to decide whether a build is necessary. This problem commonly requires slightly different code for each repository (due to variance in directory structures and language/tooling choices).\nTakeaways GitOps - It all starts with the Repo 1 Config Repo : N Code Repos Code Repo (for developing) --produces--\u0026gt; Built Software Config Repo (for deploying) --produces--\u0026gt; Running Software "});index.add({'id':2,'href':'/docs/cloud-native-handbook/development/','title':"Development",'content':"Development All development should occur in a \u0026ldquo;code repo\u0026rdquo;. This is a repository where the sole purpose is to produce deployable software. In the cloud native world, this means the goal is to push container images to a registry.\nBranching Strategy The branching strategy described here mirrors oneflow. We feel like it is a good fit for most teams. A single long-lived branch keeps merging simple. Git tagging triggers container image tagging. Deployment is decoupled, avoiding the complexity of maintaining environment-specific branches.\n 1. Features To add new features, first create a feature/\u0026lt;feature-name\u0026gt; branch.\n# From the main branch (git checkout main)... git checkout -b feature/my-feature Add your code changes and any documentation updates. Push the branch.\ngit push -u origin feature/my-feature Create a Pull Request from your branch feature/my-feature into main.\nNow the git hosting service you are using should require the following before enabling a merge:\n Unit tests End-to-end tests Linting Image build success Human approval  Use the following checklist when approving pull requests:\n [✓] Code logic is easy to follow. [✓] Code follows conventions. [✓] Dense code has commented explanations. [✓] Unit/e2e tests for feature exist. [✓] User documentation is updated (if seperate from repo, add link in PR description).  Once the merge occurs, the feature branch should be automatically removed to keep the repository clean.\n2. Releases To prepare for releasing software (updating versions / documentation / etc), create a release/\u0026lt;semantic-version\u0026gt; branch.\n# From the main branch (git checkout main)... git checkout -b release/2.3.0 This release branch should follow semantic versioning:\n Backwards compatible changes (same major version, a new minor version, zero\u0026rsquo;ed patch version): 2.3.0 Non-Backwards compatible changes (a new major version, zero\u0026rsquo;ed minor/patch versions): 3.0.0  Once all of the documentation and version updates are done, push the branch.\ngit push -u origin release/2.3.0 All of the same automated checks that apply to feature branches should be run before the release branch can be merged. The human approval checklist should include:\n [✓] Changelog is updated. [✓] Any version references in config/code are updated. [✓] User documentation is updated.*   *A note about example snippets in documentation\u0026hellip;\nExamples of how to run the software (command line snippets, etc) should be ran manually or automated. Often these snippets go out of date. This is the perfect checkpoint for making sure they still work (as opposed to feature pull requests which happen more frequently).\n To trigger the release, an admin creates a git tag on the most recent commit in the release branch. The tag should match the branch version, prefixed with a \u0026ldquo;v\u0026rdquo; (i.e. v2.3.0). In GitHub this is done by creating a \u0026ldquo;release\u0026rdquo; (which creates a tag). In BitBucket there is a button for creating tags. Upon creation of the tag, the same automated checks that are in place for Pull Request should be run, followed by an automated push of the built container image to a registry. The image tag should match the git tag.\nThe software is now released. Now merge the release branch back into main to ensure all documentation updates make their way into future releases. The release branch should be automatically deleted after merged.\n A note about git tags\u0026hellip;\nGit tags are used to name a commit. Unlike branches, which are intended to point to an ever-growing list of commits, tags are intended to always point at the same commit.\n 3. Patches Patches are needed when defects are found in released software. Commonly, the main branch has progressed past the current released software when the bug is found. The goal is to quickly fix the bug without hurriedly releasing a new set of features. This is commonly referred to as a \u0026ldquo;hotfix\u0026rdquo;.\nTo start this process, first create a new patch/\u0026lt;semantic-version\u0026gt; branch off of the current released tag. The semantic version should be the same as the current release, except it should have the patch version bumped.\ngit checkout -b patch/2.3.1 v2.3.0 After fixing the bug, follow the same procedure used in a release branch: create a pull request, create a tag matching the branch version, and merge the hotfix back into the main branch to ensure it does not re-appear in future releases.\nCI Systems In a cloud native world, all tests and builds should occur in containers. This means that your CI system does not need to be very complex. The biggest complexity here will come from linking the CI system to the repository (auth with service accounts, defining branch merging requirements, etc). Prefer integrated CI systems when possible to avoid the complexity of linking these two systems (i.e. GitHub workflows, GitLab pipelines).\nTakeaways 1 Long lived branch: \u0026quot;main\u0026quot; 3 Short-lived branch types (all merge to main): - \u0026quot;feature/\u0026lt;name\u0026gt;\u0026quot; - \u0026quot;release/\u0026lt;semver\u0026gt;\u0026quot; (new major/minor version, patch version = 0) - \u0026quot;patch/\u0026lt;semver\u0026gt;\u0026quot; (same major/minor version, patch version ++) * Pull requests trigger testing \u0026amp; image builds. * Tagging triggers testing \u0026amp; image builds/pushes. "});index.add({'id':3,'href':'/docs/cloud-native-handbook/deployment/','title':"Deployment",'content':"Deployment Since Kubernetes is a declarative system, the act of deploying software translates to pushing Kubernetes configuration manifests into a cluster. These manifests contain descriptions of applications that should be running (what container image, how many replicas, any environment variables, dns names, etc). The cluster will then work to make sure what was declared becomes a reality.\nBranching Strategy All manifests should be version controlled in a git repository (referred to here as a config repo). This usually translates to a \u0026ldquo;cluster config repo\u0026rdquo; for the platform team and at least one \u0026ldquo;app config repo\u0026rdquo; for each app team. Any commit to a config repo should result in the those manifests being applied into a cluster. The following branching strategy should work for most use-cases.\n Environment Promotion There are two main concerns when promoting software across environments:\n Isolation - Seperating changes across environments Parity / Variety - Minimizing / efficiently accounting for differences between environments  Isolation Environment isolation is addressed using branching. On the path-to-production pull requests are used to promote changes across environments. These merges should be fast-forward-only, eliminating merge conflicts.\nParity / Variety As software moves from non-production to production it is important maintain the maximum level of parity possible. One way to do this is by sharing a base configuration and maintaining diffs (patches) to account for environmental variance. Kustomize is the tool of choice for doing this. Environments are seperated out by directories with all commonality living in base:\nbase/ # Common config across all environments staging/ # Deployed from \u0026quot;staging\u0026quot; branch (references base/) production/ # Deployed from \u0026quot;production\u0026quot; branch (references base/) A base/ directory is used to account for commonality (parity) across environments. Changes that are to be promoted from non-production to production should be made in base (i.e. updated image version). As the base/ changes make their way into environment-specific branches, these changes will be promoted.\nIndividual \u0026lt;environment\u0026gt;/ directories are used to account for variances that should always exist across environments (i.e. replica count / resource allocation). These variations are usually maintained in the form of patches on the base resources.\n Why not use staging as a base for production?\nTools like kustomize do not allow for removing resources from a common base for the sake of simplicity. For this reason, base/ is used to represent only the commonality across environments.\n CD System The action of deploying can be done either through a pipeline (i.e. kubectl apply in a Jenkins job) or a GitOps engine (i.e. ArgoCD, FluxCD).\nA GitOps engine is preferable to a pipeline because it provides active reconciliation as opposed to point-in-time reconciliation. To address config drift the engine constantly ensures that the state of a cluster matches the state of your repository.\n The Kubernetes API and GitOps\u0026hellip;\nThe Kubernetes API Server is a REST API that allows clients to watch for changes to resources. This allows a GitOps engine to sync API resources with corrsponding files that live in a Git repo and quickly remedy config drift.\n Repo Scope The scope of a config repo should be determined by identifying a bounded context (which should ideally correspond to a team of people). Using GitOps objects to point to other repositories, a tree structure emerges. The root of the tree is usually a repository that the cluster operations team manages. Commonly, all cluster-level configuration is held in another repo, maintained by the same team. Application-specific repositories are also referenced, delegating access to app teams to deploy specific resources (i.e. Deployments, Services, etc.) into whitelisted namespaces.\n Takeaways * Environments seperated by directory within repo. * Staging/Production/etc. environments seperated by branch (and directory). * Fast-forward branch merges: staging --\u0026gt; prod "});index.add({'id':4,'href':'/docs/cloud-native-handbook/exercise-deploy/','title':"Exercise: Deploying",'content':"Exercise - Deploying Stuff to Kubernetes Prereqs Make sure you have the following tools installed and up to date:\n  docker - For building containers and running k8s with kind.  kind - For creating local k8s clusters.  kubectl - For interacting with k8s.  kustomize - For templating k8s manifests.  Setup Define the environment.\nexport ENV=non-prod export CLUSTER=non-east-a Create a local cluster representing this environment.\nkind create cluster --name $CLUSTER Bootstrap the Cluster Install the GitOps engine.\nkustomize build \u0026#34;https://github.com/codeformio/k8s-cluster-config/base/argocd?ref=$ENV\u0026#34; | kubectl apply -f - Configure the GitOps engine.\nkustomize build \u0026#34;https://github.com/codeformio/k8s-gitops-config/$ENV/$CLUSTER?ref=$ENV\u0026#34; | kubectl apply -f - Login to UI The ArgoCD UI can be used to view the application. By default the password for the admin user is set to the server\u0026rsquo;s pod name.\nkubectl get pods -n argocd Port forward and login by visiting https://localhost:8080.\nkubectl port-forward -n argocd svc/argocd-server 8080:443 Port forward the application\u0026rsquo;s port to see it respond.\nkubectl port-forward -n greeter svc/hello-api 8000:80 In another terminal\u0026hellip;\ncurl localhost:8000 -v Update an Application TODO\nDeploy an Updated Application TODO\nCleanup Delete the local cluster.\nkind delete cluster --name $CLUSTER "});index.add({'id':5,'href':'/mentoring/','title':"Mentoring",'content':"Mentoring We are currently actively mentoring individuals who want to start or grow their tech career. If any of the following sounds like you, we can help!\n I am new, where to start? I want to learn the Go programming language. What is a VM? Diving into dontainers / docker. What is kubernetes? What is a REST API? What is gRPC? Can you review my code? I would love a second opinion on my personal project.  The best way to get in touch is via social media:\n Twitter: https://twitter.com/nickstogner LinkedIn: https://www.linkedin.com/in/nstogner/  "});index.add({'id':6,'href':'/docs/cloud-native-handbook/teams/','title':"Teams",'content':"Teams Delegation of Resource Management When organizing teams and processes, it helps to think of Kubernetes manifests as either application or cluster related:\n  Application-related manifests are namespaced, some common ones being:\n Deployment (What container should be running) Service (Service discovery / cluster DNS) ConfigMap (Define application confiuration files and env vars)    Some examples of cluster-concerns / administrative manifests include:\n Namespace (A group of other resources) ResourceLimit (How much cpu/memory can be used in a Namespace) NetworkPolicy (What apps can talk to eachother) ClusterRole/ClusterRoleBinding (Access control, what k8s users can see/do)    Typically, cluster operations teams have the ability to manage what namespaces (and associated restrictions) exist. They would then delegate access to configure application-related manifests into those namespaces to individual app teams.\nUser Access Control TODO\n"});index.add({'id':7,'href':'/posts/getting-started-with-cloud-init/','title':"Getting Started with Cloud Init",'content':"In this guide we are going to use cloud-init to bootstrap an apache server on AWS EC2.\nIntroduction Cloud init is a service that comes installed on newer OS distributions. It is able to consume cloud-config files and execute them during the very first boot of a server. Cloud-config files use a declarative syntax (YAML format) to specify common configuration tasks such as:\n Configuring SSH keys Settting up trusted CA certs Define users Run scripts etc.  Prerequisites  AWS CLI installed \u0026amp; configured jq (brew install jq)  Steps Create a key pair to allow you to login to the VM you create. NOTE: You may already have one but we will go ahead and remove this one later anyways.\nexport KEY_NAME=cloud-init-tutorial export KEY_FILE=${KEY_NAME}.pem aws ec2 create-key-pair --key-name ${KEY_NAME} --query \u0026#39;KeyMaterial\u0026#39; --output text \u0026gt; ${KEY_FILE} chmod 400 ${KEY_FILE} Create a security group that allows for SSH access from anywhere.\nexport SEC_GRP_ID=`aws ec2 create-security-group --group-name CloudInitTutorial --description \u0026#34;SSH from anywhere\u0026#34; --output json | jq -r \u0026#39;.GroupId\u0026#39;` aws ec2 authorize-security-group-ingress \\  --group-id ${SEC_GRP_ID} \\  --protocol tcp \\  --port 22 \\  --cidr 0.0.0.0/0 Define the cloud-config file we will use to install apache.\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt;\u0026gt; cloud-config.txt #cloud-config repo_update: true repo_upgrade: all packages: - httpd - mariadb-server runcmd: - [ sh, -c, \u0026#34;amazon-linux-extras install -y lamp-mariadb10.2-php7.2 php7.2\u0026#34; ] - systemctl start httpd - sudo systemctl enable httpd - [ sh, -c, \u0026#34;usermod -a -G apache ec2-user\u0026#34; ] - [ sh, -c, \u0026#34;chown -R ec2-user:apache /var/www\u0026#34; ] - chmod 2775 /var/www - [ find, /var/www, -type, d, -exec, chmod, 2775, {}, \\; ] - [ find, /var/www, -type, f, -exec, chmod, 0664, {}, \\; ] - [ sh, -c, \u0026#39;echo \u0026#34;\u0026lt;?php phpinfo(); ?\u0026gt;\u0026#34; \u0026gt; /var/www/html/phpinfo.php\u0026#39; ] EOF Create the VM. Note the --user-data flag we are passing. This is where we are specifying the cloud-init configuration. AWS knows to interpret this as a cloud-init file because of the first line in the file: #cloud-config. See more examples of cloud configs here.\nexport INSTANCE_ID=`aws ec2 run-instances \\ \t--image-id ami-00c03f7f7f2ec15c3 \\ \t--count 1 \\ \t--instance-type t2.micro \\ \t--key-name ${KEY_NAME} \\ \t--security-group-ids ${SEC_GRP_ID} \\ \t--user-data file://cloud-config.txt \\ \t| jq -r \u0026#39;.Instances[0].InstanceId\u0026#39;` Grab the public DNS name of the VM.\nexport INSTANCE_DNS=`aws ec2 describe-instances --filters \u0026#34;Name=instance-id,Values=${INSTANCE_ID}\u0026#34; | jq -r \u0026#39;.Reservations[0].Instances[0].PublicDnsName\u0026#39;` Login to the VM and test that our cloud-init configuration worked. Note: you may have to wait for the instance to initialize.\nssh -i ${KEY_FILE} ec2-user@${INSTANCE_DNS} wget -O - http://localhost:80/phpinfo.php Cleanup # Terminate instance. aws ec2 terminate-instances --instance-ids ${INSTANCE_ID} # Delete security group (after instance is terminated). until aws ec2 delete-security-group --group-id ${SEC_GRP_ID}; do echo 'Attempting to delete security group again...' \u0026amp;\u0026amp; sleep 1; done # Delete our key pair we created. aws ec2 delete-key-pair --key-name ${KEY_NAME} rm ${KEY_FILE} Conclusion While cloud-init can be used to configure almost any aspect of a VM instance, you want to be sure you are using the right tool for the job. For instance, installing packages on a VM might be a job better suited for a tool like packer which builds reusable images. This eliminates boot-time steps, reducing spin-up time. Reach for cloud-init in your toolbox when it does not make sense to bake the configuration into an image, for instance: joining a VM to a Kubernetes cluster.\nHappy DevOp\u0026rsquo;ing!\n"});index.add({'id':8,'href':'/posts/google-cloud-architect-certification-notes/','title':"Notes on the Google Cloud Architect Certification",'content':"When Google Cloud launched their Kubernetes platform (GKE) back in 2015 I signed up for the free trial. Today my free credits are long gone, but I still run and launch new clusters on a weekly basis. After putting it off for a while, I recently decided to formalize my experience by getting the GCP Architect certification. In this post I detail some of the resources I used to prepare for the exam.\nTo my surprise, this exam is not just a test of concepts specific to Google\u0026rsquo;s products. While a thorough understanding of GCP is required, the exam is also designed to assess your proficiency as a software architect in general. This makes giving study advice difficult since the topics are not just restrained to the GCP documentation. With that disclaimer, here is a list of some of the material I used to prepare for the exam.\nGeneral Resources These overviews and courses are a good starting point.\n  Certification Overview - What are you getting yourself into?  Exam Guide/Outline - Bullet points of topics covered  Coursera Specialization - Covers various GCP products (\u0026gt;1 month of material)  Coursera Exam Prep - Gives you an overview of the exam and strategies for additional preparation  Practice Exam - After completing, read the explanations to get an idea for the line of reasoning the authors take in their questions and solutions  Miscellaneous Topics The following topics are not comprehensive and they are in no particular order. I included them to provide an example of the depth/breadth of knowledge you will need to know.\nKubernetes   Creating clusters \u0026amp; running workloads  Deploying / updating applications using kubectl  Networking   VPN Concepts (gateways \u0026amp; network topography)  Firewall concepts  IAM  How resource hierarchies work with IAM  BigQuery  Best practices around schema design  Table partitioning and data expiration  Access control (roles) \u0026amp; how jobs are billed  Datastore   Best practices around querying (multiple gets / batch get / query filters)  Index management  Cloud Storage   Methods of data transfer (what is considered a large dataset?) Uploading large amounts of data using a Transfer Appliance (familiarity with the data transfer process)  Object lifecycle management  Using encryption keys  CI/CD  High level familiarity with spinnaker \u0026amp; jenkins  Best of Luck! Like most people in software I find myself a little suspicious of the value of most certifications. However I believe this particular certification is worth the time investment for anyone building on top of GCP. Preparing for the exam forced me fill in my GCP knowledge gaps and provided insight into the various use-cases that Google Cloud products are intended to solve.\n"});index.add({'id':9,'href':'/posts/simple-golang-retry-function/','title':"Simple Golang Retry Function",'content':"Adding retry policies in your software is an easy way to increase resiliency. This is especially useful when making HTTP requests or doing anything else that has to reach out across the network.\nIf at first you don’t succeed, try, try again. In go code, that translates to:\nfunc retry(attempts int, sleep time.Duration, fn func() error) error { if err := fn(); err != nil { if s, ok := err.(stop); ok { // Return the original error for later checking \treturn s.error } if attempts--; attempts \u0026gt; 0 { time.Sleep(sleep) return retry(attempts, 2*sleep, fn) } return err } return nil } type stop struct { error } The retry function recursively calls itself, counting down attempts and sleeping for twice as long each time (i.e. exponential backoff). This technique works well until the situation arises where a good number of clients start their retry loops at roughly the same time. This could happen if a lot of connections get dropped at once. The retry attempts would then be in sync with each other, creating what is known as the Thundering Herd problem. To prevent this, we can add some randomness by inserting the following lines before we call time.Sleep:\njitter := time.Duration(rand.Int63n(int64(sleep))) sleep = sleep + jitter/2 The improved, jittery version:\nfunc init() { rand.Seed(time.Now().UnixNano()) } func retry(attempts int, sleep time.Duration, f func() error) error { if err := f(); err != nil { if s, ok := err.(stop); ok { // Return the original error for later checking \treturn s.error } if attempts--; attempts \u0026gt; 0 { // Add some randomness to prevent creating a Thundering Herd \tjitter := time.Duration(rand.Int63n(int64(sleep))) sleep = sleep + jitter/2 time.Sleep(sleep) return retry(attempts, 2*sleep, f) } return err } return nil } type stop struct { error } There are two options for stopping the retry loop before all the attempts are made:\n Return nil Return a wrapped error: stop{err}  Choose option #2 when an error occurs where retrying would be futile. Consider most 4XX HTTP status codes. They indicate that the client has done something wrong and subsequent retries, without any modification to the request will result in the same response. In this case we still want to return an error so we wrap the error in the stop type. The actual error that is returned by the retry function will be the original non-wrapped error. This allows for later checks like err == ErrUnauthorized.\nTake a look at the following implementation for retrying a HTTP request. Note: In this case, there are only 2 additional lines needed for adding in the retry policy to the existing DeleteThing function (lines 14 and 34).\n// DeleteThing attempts to delete a thing. It will try a maximum of three times. func DeleteThing(id string) error { // Build the request \treq, err := http.NewRequest( \u0026#34;DELETE\u0026#34;, fmt.Sprintf(\u0026#34;https://unreliable-api/things/%s\u0026#34;, id), nil, ) if err != nil { return fmt.Errorf(\u0026#34;unable to make request: %s\u0026#34;, err) } // Execute the request \treturn retry(3, time.Second, func() error { resp, err := http.DefaultClient.Do(req) if err != nil { // This error will result in a retry \treturn err } defer resp.Body.Close() s := resp.StatusCode switch { case s \u0026gt;= 500: // Retry \treturn fmt.Errorf(\u0026#34;server error: %v\u0026#34;, s) case s \u0026gt;= 400: // Don\u0026#39;t retry, it was client\u0026#39;s fault \treturn stop{fmt.Errorf(\u0026#34;client error: %v\u0026#34;, s)} default: // Happy \treturn nil } }) } Updates\n[June 1, 2017] Edited to add the jitter example\n"});index.add({'id':10,'href':'/posts/golang-wrapping-http-response-writer-for-middleware/','title':"Go: Wrapping http.ResponseWriter with Middleware",'content':"Using middleware provides a clean way to reduce code duplication when handling HTTP requests in Go. By utilizing the standard handler signature, func(w http.ResponseWriter, r *http.Request) we can write functions that are easily dropped into any Go web service. One of the most common uses of middleware is to provide request/response logging. In order to log responses we will need to capture the status code that was written by a nested handler.\nSince we don\u0026rsquo;t have direct access to this status code, we will wrap the ResponseWriter that is passed down:\ntype statusRecorder struct { http.ResponseWriter status int } func (rec *statusRecorder) WriteHeader(code int) { rec.status = code rec.ResponseWriter.WriteHeader(code) } func logware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { // Initialize the status to 200 in case WriteHeader is not called \trec := statusRecorder{w, 200} next.ServeHTTP(\u0026amp;rec, r) log.Printf(\u0026#34;response status: %v\\n\u0026#34;, rec.status) }) } Because the http.ResponseWriter type is an interface we can pass a custom type to subsequent handlers as long as it implements all of the defined methods:\ntype ResponseWriter interface { Header() Header Write([]byte) (int, error) WriteHeader(int) } By using composition we only need to explicitly implement the WriteHeader method that we want to modify. This is because we have embedded the ResponseWriter type in our struct so all of its methods get promoted and can be called on statusRecorder.\nNote: We pass a pointer into ServeHTTP so that nested handlers can change the status field that we later reference.\nUsing the middleware is simple:\nfunc main() { http.ListenAndServe(\u0026#34;:8080\u0026#34;, logware( http.HandlerFunc(handle), ), ) } func handle(w http.ResponseWriter, r *http.Request) { w.WriteHeader(201) w.Write([]byte(\u0026#34;Accepted\u0026#34;)) } The above handle function calls two methods:\n WriteHeader: This calls our statusRecorder.WriteHeader method which records the 201 status for logging and then calls the original ResponseWriter.WriteHeader method. Write: This calls the original ResponseWriter.Write method directly.  Why do we choose to store the status in our struct rather than having the WriteHeader method do the logging? Calling WriteHeader inside of a handler is optional. An implicit 200-OK status is sent to the client if the status code is not set. In this situation the logging call would be missed.\nHappy coding, and be on the lookout for other places to use composition and interfaces to simplify your Go code.\n"});index.add({'id':11,'href':'/docs/','title':"Docs",'content':""});index.add({'id':12,'href':'/posts/','title':"Posts",'content':"\n"});})();